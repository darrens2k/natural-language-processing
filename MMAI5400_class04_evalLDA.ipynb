{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC-Myl9iXnNm"
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA): model selection and evaluation\n",
    "\n",
    "Modified from [Evaluate Topic Models: Latent Dirichlet Allocation (LDA)](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0).\n",
    "\n",
    "LDA is used to cluster documents in particular topics. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. \n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial. \n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. \n",
    "\n",
    "We will use [Gensim](https://radimrehurek.com/gensim/) for topic modelling with LDA. Gensim is a popular open-source library for unsupervised topic modeling and natural language processing.\n",
    "\n",
    "Gensim is implemented in Python and Cython, the latter for improved performance. Gensim is designed to handle large text collections using data streaming and incremental online algorithms, which differentiates it from most other machine learning software packages that target only in-memory processing. \n",
    "\n",
    "**We will evaluate and select topic models using a measure of topic coherence.**\n",
    "\n",
    "### Why evaluate topic models?\n",
    "\n",
    "Probabilistic topic models, such as LDA, are popular tools for text analysis, providing both a predictive and latent topic representation of the corpus. There is a longstanding assumption that the latent space discovered by these models is meaningful and useful, but evaluating such assumptions is challenging due to its unsupervised training process. Since there is a no-gold standard list of topics to compare against every corpus, we cannot compute the normal performance metrics we do for supervised learning.\n",
    "\n",
    "However, it is still critically important to identify if a trained model is objectively good or bad, as well to compare different models/methods, and to do so, we need a quality measure. While implicit knowledge and \"eyeballing\" are popular, they are not objective approaches that can be applied systematically. We need something better, that perferably captures the model's quality in a single metric that can be maximized and compared. \n",
    "\n",
    "These are the commonly used approaches for evaluation:\n",
    "\n",
    "**Eye Balling Models**\n",
    "- Top N words\n",
    "- Topics / Documents\n",
    "\n",
    "**Intrinsic Evaluation Metrics**\n",
    "- Capturing model semantics\n",
    "- Topics interpretability\n",
    "\n",
    "**Human Judgements**\n",
    "- [What is a topic?](https://proceedings.neurips.cc/paper/2009/file/f92586a25bb3145facd64ab20fd554ff-Paper.pdf)\n",
    "\n",
    "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
    "- Is model good at performing predefined tasks, such as clustering \n",
    "\n",
    "A big problem is that natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to de-ambiguite reduces the language to an unnatural form. Nevertheless, in order to use a single quality metric we will have to accept such risks.\n",
    "\n",
    "### What is Topic Coherence?\n",
    "\n",
    "Perplexity is often used as an example of an intrinsic evaluation measure. It comes from the language modeling community and aims to capture how surprised a model is of new data it has not seen before. It is measured as the normalized log-likelihood of a held-out test set.\n",
    "\n",
    "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
    "\n",
    "[However, past research has shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated.](http://qpleple.com/perplexity-to-evaluate-topic-models/) And that served as a motivation for more work trying to model the human judgment, and thus `Topic Coherence`.\n",
    "\n",
    "The topic coherence concept combines a number of papers into one framework that allows evaluating the coherence of topics inferred by a topic model. But,\n",
    "\n",
    "#### What is topic coherence?\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
    "\n",
    "#### What is coherence?\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is \"the game is a team sport\", \"the game is played with a ball\", \"the game demands great physical efforts\"\n",
    "\n",
    "### Coherence Measures\n",
    "\n",
    "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity. See explainations in the paper by [Syed and Spruit](http://www.saf21.eu/wp-content/uploads/2017/09/5004a165.pdf) the [paper by Röder et al](https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) and in [this blog post](https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227).\n",
    "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence. See explainations in the [paper by Röder et al](https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf).\n",
    "3. [`C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words.](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)\n",
    "4. [`C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure.](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)\n",
    "5. [`C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI).](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf)\n",
    "6. `C_a` is baseed on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHhLF0KwXnNq"
   },
   "source": [
    "### Model Implementation\n",
    "1. Loading Data\n",
    "2. Data Cleaning\n",
    "3. Phrase Modeling: Bi-grams and Tri-grams\n",
    "4. Data Transformation: Corpus and Dictionary\n",
    "5. Base Model\n",
    "6. Hyper-parameter Tuning\n",
    "7. Final model\n",
    "8. Visualize Results\n",
    "\n",
    "** **\n",
    "\n",
    "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
    "\n",
    "Let’s start by looking at the content of the file\n",
    "\n",
    "** **\n",
    "#### Step 0: Install the latest version of Gensim\n",
    "** **\n",
    "\n",
    "An old (and buggy) version of Gensim is installed by default on Google Colab. Please upgrade to the latest version using the command above and restart the runtime so that the new version is loaded.\n",
    "\n",
    "While we're at it, let's also install [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/readme.html#:~:text=pyLDAvis%20is%20designed%20to%20help,an%20interactive%20web%2Dbased%20visualization.), a package that helps users interpret the topics in a topic model. The package extracts information from an LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuZZJ8OmX78t",
    "outputId": "15cf52ea-301f-4267-df61-c05d5abfd7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: pyldavis in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (1.2.0)\n",
      "Requirement already satisfied: funcy in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (1.2.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (3.1.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (2.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (63.2.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyldavis) (2.8.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=2.0.0->pyldavis) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=2.0.0->pyldavis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=1.0.0->pyldavis) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->pyldavis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eG4NbDn9XnNt"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from packaging import version\n",
    "assert version.parse(\"3.7\") < version.parse(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxmPIUC8ibRo"
   },
   "source": [
    "** **\n",
    "#### Step 1: Loading Data\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "O4GHMPJtg-Ps",
    "outputId": "64e1a97f-0f29-4651-82ad-587f4ddbeff8"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for fn in uploaded.keys():\n",
    "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#       name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qxvohYUDXnNr",
    "outputId": "94812022-d827-46fa-f451-4ef63207b9ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3081</td>\n",
       "      <td>Localizing Bugs in Program Executions\\nwith Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6184</td>\n",
       "      <td>Combinatorial Energy Learning for Image\\nSegme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4615</td>\n",
       "      <td>A multi-agent control framework for co-adaptat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3936</td>\n",
       "      <td>Distributed Non-Stochastic Experts\\n\\nVarun Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986</td>\n",
       "      <td>Learning Rankings via Convex Hull Separation\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         paper_text\n",
       "0        3081  Localizing Bugs in Program Executions\\nwith Gr...\n",
       "1        6184  Combinatorial Energy Learning for Image\\nSegme...\n",
       "2        4615  A multi-agent control framework for co-adaptat...\n",
       "3        3936  Distributed Non-Stochastic Experts\\n\\nVarun Ka...\n",
       "4        1986  Learning Rankings via Convex Hull Separation\\n..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data into papers\n",
    "# bigger data files on canvas to get a better performing model\n",
    "papers = pd.read_csv(R\"C:\\Users\\darre\\Downloads\\papers_100.csv\")\n",
    "\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QBI01ImXnNt"
   },
   "source": [
    "** **\n",
    "#### Step 2: Data Cleaning\n",
    "** **\n",
    "\n",
    "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-q7CAuhtXnNu",
    "outputId": "78b13c33-44be-433e-c0a3-b2e871366d73"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Localizing Bugs in Program Executions\\nwith Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combinatorial Energy Learning for Image\\nSegme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A multi-agent control framework for co-adaptat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distributed Non-Stochastic Experts\\n\\nVarun Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning Rankings via Convex Hull Separation\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_text\n",
       "0  Localizing Bugs in Program Executions\\nwith Gr...\n",
       "1  Combinatorial Energy Learning for Image\\nSegme...\n",
       "2  A multi-agent control framework for co-adaptat...\n",
       "3  Distributed Non-Stochastic Experts\\n\\nVarun Ka...\n",
       "4  Learning Rankings via Convex Hull Separation\\n..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "# sample only 100 papers\n",
    "# papers = papers.sample(100)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc_8tZi4XnNu"
   },
   "source": [
    "##### Remove punctuation/lower casing\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaJG3KxzXnNv",
    "outputId": "7b125d61-bcdb-4742-9504-d20a51988b00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    localizing bugs in program executions\\nwith gr...\n",
       "1    combinatorial energy learning for image\\nsegme...\n",
       "2    a multi-agent control framework for co-adaptat...\n",
       "3    distributed non-stochastic experts\\n\\nvarun ka...\n",
       "4    learning rankings via convex hull separation\\n...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DB7ESKWBmTTI",
    "outputId": "638879c7-54f7-4ab5-b600-2d8d7303774f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "      <th>paper_text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Localizing Bugs in Program Executions\\nwith Gr...</td>\n",
       "      <td>localizing bugs in program executions\\nwith gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combinatorial Energy Learning for Image\\nSegme...</td>\n",
       "      <td>combinatorial energy learning for image\\nsegme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A multi-agent control framework for co-adaptat...</td>\n",
       "      <td>a multi-agent control framework for co-adaptat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Distributed Non-Stochastic Experts\\n\\nVarun Ka...</td>\n",
       "      <td>distributed non-stochastic experts\\n\\nvarun ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning Rankings via Convex Hull Separation\\n...</td>\n",
       "      <td>learning rankings via convex hull separation\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_text   \n",
       "0  Localizing Bugs in Program Executions\\nwith Gr...  \\\n",
       "1  Combinatorial Energy Learning for Image\\nSegme...   \n",
       "2  A multi-agent control framework for co-adaptat...   \n",
       "3  Distributed Non-Stochastic Experts\\n\\nVarun Ka...   \n",
       "4  Learning Rankings via Convex Hull Separation\\n...   \n",
       "\n",
       "                                paper_text_processed  \n",
       "0  localizing bugs in program executions\\nwith gr...  \n",
       "1  combinatorial energy learning for image\\nsegme...  \n",
       "2  a multi-agent control framework for co-adaptat...  \n",
       "3  distributed non-stochastic experts\\n\\nvarun ka...  \n",
       "4  learning rankings via convex hull separation\\n...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-26DAD6XnNv"
   },
   "source": [
    "##### Tokenize words and further clean-up text\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VZI78vyXnNw",
    "outputId": "a010d74b-4ab3-4237-e606-3c65d68bf119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['localizing', 'bugs', 'in', 'program', 'executions', 'with', 'graphical', 'models', 'valentin', 'dallmeier', 'saarland', 'university', 'saarbruecken', 'germany', 'dallmeier', 'csuni', 'saarlandde', 'laura', 'dietz', 'max', 'planck', 'institute', 'for', 'computer', 'science', 'saarbruecken', 'germany', 'dietz', 'mpi', 'infmpgde']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2R5P5tFyXnNw"
   },
   "source": [
    "** **\n",
    "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
    "** **\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
    "\n",
    "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "o5EfbOliXnNw"
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icwbR4wIXnNx"
   },
   "source": [
    "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBcmO_JrXnNx",
    "outputId": "5610923b-a274-475f-e636-ebb6990d1997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\darre\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QwUcdQEmXnNx"
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM2FxLdKXnNy"
   },
   "source": [
    "Let's call the functions in order.\n",
    "\n",
    "But first download `en_core_web_sm`, a pre-trained English Spacy pipeline optimized for CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpXjXOufXnNy",
    "outputId": "15c802de-429e-4cd6-dcb5-66c185fe99b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python310\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BWRcDFqXnNz",
    "outputId": "a7ffd76c-94a0-41e8-a34c-7c5e44fccd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.0-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "     ---------------------------------------- 12.1/12.1 MB 8.7 MB/s eta 0:00:00\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.9/45.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "     -------------------------------------- 395.8/395.8 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting thinc<8.3.0,>=8.1.8\n",
      "  Downloading thinc-8.2.1-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.7 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.26.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "     -------------------------------------- 122.2/122.2 kB 7.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\darre\\appdata\\roaming\\python\\python310\\site-packages (from spacy) (23.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "     ------------------------------------- 481.9/481.9 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.2-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.0/50.0 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic-core==2.10.1\n",
      "  Downloading pydantic_core-2.10.1-cp310-none-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 9.6 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\darre\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Collecting cloudpathlib<0.16.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.15.1-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 43.9/43.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\darre\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, pathy, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.15.1 confection-0.1.3 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 pathy-0.10.2 preshed-3.0.9 pydantic-2.4.2 pydantic-core-2.10.1 spacy-3.7.0 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.1 typer-0.9.0 wasabi-1.1.2 weasel-0.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\darre\\Documents\\GitHub\\natural-language-processing\\MMAI5400_class04_evalLDA.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/darre/Documents/GitHub/natural-language-processing/MMAI5400_class04_evalLDA.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data_words_bigrams \u001b[39m=\u001b[39m make_bigrams(data_words_nostops)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/darre/Documents/GitHub/natural-language-processing/MMAI5400_class04_evalLDA.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/darre/Documents/GitHub/natural-language-processing/MMAI5400_class04_evalLDA.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m, disable\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mner\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/darre/Documents/GitHub/natural-language-processing/MMAI5400_class04_evalLDA.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Do lemmatization keeping only noun, adj, vb, adv\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/darre/Documents/GitHub/natural-language-processing/MMAI5400_class04_evalLDA.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m data_lemmatized \u001b[39m=\u001b[39m lemmatization(data_words_bigrams, allowed_postags\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mNOUN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mADJ\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVERB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mADV\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:50\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     27\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     28\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     34\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     35\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     51\u001b[0m         name,\n\u001b[0;32m     52\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     53\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     54\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     55\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     56\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     57\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\darre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "# strange error, library might need a complicated install\n",
    "# unchanged code should just work in colab\n",
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcoUhS9xpDLM",
    "outputId": "e73297d6-fa32-4c4a-b6a9-13dc08fcf7e2"
   },
   "outputs": [],
   "source": [
    "len(data_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqRx5PqSXnNz"
   },
   "source": [
    "** **\n",
    "#### Step 4: Data transformation: Corpus and Dictionary\n",
    "** **\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary (`id2word`) and the `corpus`. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SaqSe9xXnN0",
    "outputId": "5e6ab9a8-c82e-4cdf-8e24-160123648c6f"
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnv-PZbtXnN0"
   },
   "source": [
    "** **\n",
    "#### Step 5: Base Model \n",
    "** **\n",
    "\n",
    "We have everything required to train the base LDA model. In addition to the corpus and dictionary, we need to provide the number of topics as well. Apart from that, `alpha` and `eta` are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to `1.0/num_topics` prior (we'll use default for the base model).\n",
    "\n",
    "`chunksize` controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
    "\n",
    "`passes` controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". `iterations` is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of `passes` and `iterations` high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DxqrNUFXnN1"
   },
   "outputs": [],
   "source": [
    "# not sure but alpha and eta might be different alphabets. \n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFXy_zEpXnN1"
   },
   "source": [
    "** **\n",
    "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "We can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SpjcPU8PXnN1",
    "outputId": "29cb2aa4-94e9-4338-e155-f3892ab34bb5"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFUrKaMXXnN2"
   },
   "source": [
    "#### Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Let's calculate the baseline coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCXlNzCWXnN2",
    "outputId": "40704402-11a7-4ed7-cfa2-998b03f5d4df"
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_c_v = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_model_u_mass = CoherenceModel(model=lda_model, corpus=corpus, dictionary=id2word, coherence='u_mass')\n",
    "coherence_c_v = coherence_model_c_v.get_coherence()\n",
    "print('Coherence Score: ', coherence_c_v)\n",
    "\n",
    "coherence_u_mass = coherence_model_u_mass.get_coherence()\n",
    "print('Coherence Score: ', coherence_u_mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQE5f77DXnN2"
   },
   "source": [
    "\n",
    "** **\n",
    "#### Step 6: Hyperparameter tuning\n",
    "** **\n",
    "First, let's differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
    "\n",
    "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters: \n",
    "- Number of Topics (`k`)\n",
    "- Dirichlet hyperparameter `alpha`: Document-Topic Density\n",
    "- Dirichlet hyperparameter `eta`: Word-Topic Density\n",
    "\n",
    "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `u_mass` as our choice of metric for performance comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnt_s9-ZXnN3"
   },
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, alpha, eta):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=alpha,\n",
    "                                           eta=eta)\n",
    "    \n",
    "    # coherence_model = CoherenceModel(model=lda_model, corpus=corpus, \n",
    "    #                                  dictionary=id2word, coherence='u_mass')\n",
    "    \n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=data_lemmatized, \n",
    "                                     dictionary=id2word, coherence='c_v')    \n",
    "    \n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIrGYMxYXnN3"
   },
   "source": [
    "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zi-Ctc45XnN3",
    "outputId": "dc822ca5-0f87-45c2-c285-1a2ec2f7f12c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 5\n",
    "max_topics = 13\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameters\n",
    "alphas = list(np.arange(0.31, 1, 0.3))\n",
    "\n",
    "# Eta parameters\n",
    "etas = list(np.arange(0.31, 1, 0.3))\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "valid_set = gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75))\n",
    "\n",
    "results = {'Number of topics': [],\n",
    "           'Alpha': [],\n",
    "           'Eta': [],\n",
    "           'Coherence': []}\n",
    "\n",
    "# Can take a long time to run\n",
    "pbar = tqdm.tqdm(total=(len(etas)*len(alphas)*len(topics_range)))\n",
    "\n",
    "# iterate through number of topics\n",
    "for k in topics_range:\n",
    "    # iterate through alpha values\n",
    "    for alpha in alphas:\n",
    "        # iterare through beta values\n",
    "        for eta in etas:\n",
    "            # get the coherence score for the given parameters\n",
    "            coherence = compute_coherence_values(corpus=valid_set, \n",
    "                                                  dictionary=id2word, \n",
    "                                                  k=k, \n",
    "                                                  alpha=alpha, \n",
    "                                                  eta=eta)\n",
    "            # Save the model results\n",
    "            results['Number of topics'].append(k)\n",
    "            results['Alpha'].append(alpha)\n",
    "            results['Eta'].append(eta)\n",
    "            results['Coherence'].append(coherence)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "results = pd.DataFrame(results)            \n",
    "results.to_csv('lda_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "j5H3A2bHrrI7",
    "outputId": "7b2a0cb0-6852-4659-d2ce-a7e843e30e36"
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "j0vEL65q_s_k",
    "outputId": "90c6f15b-7d94-4a93-ab9b-127b54b21e95"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(results['Number of topics'], results['Coherence'], 'ok')\n",
    "ax.set_xlabel('Number of topics')\n",
    "ax.set_ylabel('Coherence -- c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "15f_HEm92M0O",
    "outputId": "c667f40e-7f89-4e10-d077-16010ddc096c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(results['Number of topics'], results['Coherence'], 'ok')\n",
    "ax.set_xlabel('Number of topics')\n",
    "ax.set_ylabel('Coherence -- c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VM-ppGOmsbHn",
    "outputId": "f8fc6dae-15f9-4fbb-8113-511d7882d07f"
   },
   "outputs": [],
   "source": [
    "results.loc[results['Coherence'].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQh3D8iMXnN4"
   },
   "source": [
    "** **\n",
    "#### Step 7: Final Model\n",
    "** **\n",
    "\n",
    "Based on the above model selection, let's train the final model with parameters yielding highest coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K8yTr2CXnN4"
   },
   "outputs": [],
   "source": [
    "num_topics = 9\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       alpha=0.91,\n",
    "                                       eta=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHIBc165XnN4",
    "outputId": "37ca08c6-714a-4816-98d4-ce6f596f3f1b"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsNl4Kl2XnN5"
   },
   "source": [
    "** **\n",
    "#### Step 8: Visualize Results\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "id": "vcIw1KFKXnN5",
    "outputId": "893b038f-b74a-4fcf-b7ab-16092580f7d9"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "#import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join(f'ldavis_tuned_{num_topics}')\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute visualization prep yourself\n",
    "if True:\n",
    "\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "      \n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'ldavis_tuned_{num_topics}.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7mIUYSM-5qu"
   },
   "source": [
    "** **\n",
    "#### Step 9: Predict\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4w8YrUA8WA8",
    "outputId": "aafd7eda-9014-4bf5-a7e6-f55fd9f939ff"
   },
   "outputs": [],
   "source": [
    "unseen_doc = \"\"\"The replica method is a non-rigorous but widely-accepted \n",
    "technique from statistical physics used in the asymptotic analysis of large, random, nonlinear problems. \n",
    "This paper applies the replica method to non-Gaussian maximum a posteriori (MAP) estimation. \n",
    "It is shown that with random linear measurements and Gaussian noise, \n",
    "the asymptotic behavior of the MAP estimate of an n-dimensional\n",
    "vector ?decouples? as n scalar MAP estimators. The result is a counterpart to Guo\n",
    "and Verd?u?s replica analysis of minimum mean-squared error estimation.\n",
    "The replica MAP analysis can be readily applied to many estimators used in\n",
    "compressed sensing, including basis pursuit, lasso, linear estimation with thresholding, \n",
    "and zero norm-regularized estimation. In the case of lasso estimation\n",
    "the scalar estimator reduces to a soft-thresholding operator, \n",
    "and for zero normregularized estimation it reduces to a hard-threshold. \n",
    "Among other benefits, the replica method provides a computationally-tractable method \n",
    "for exactly computing various performance metrics including mean-squared error \n",
    "and sparsity pattern recovery probability.\"\"\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "unseen_words = gensim.utils.simple_preprocess(str(unseen_doc), deacc=True)\n",
    "unseen_words_nostops = remove_stopwords([unseen_words])\n",
    "unseen_words_bigrams = make_bigrams(unseen_words_nostops)\n",
    "unseen_lemmatized = lemmatization(unseen_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "bow_vector = id2word.doc2bow(unseen_lemmatized[0])\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(f\"Score: {score}\\t Topic: {lda_model.print_topic(index, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDMxZTK1XnN6"
   },
   "source": [
    "\n",
    "#### References:\n",
    "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
    "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
    "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
    "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
    "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "7. http://palmetto.aksw.org/palmetto-webapp/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MMAI5400_class04_evalLDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
